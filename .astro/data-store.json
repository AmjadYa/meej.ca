[["Map",1,2,9,10],"meta::meta",["Map",3,4,5,6,7,8],"astro-version","5.1.1","content-config-digest","52892818892996ca","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"site\":\"https://meej.ca\",\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[]},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":\"shiki\",\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"responsiveImages\":false},\"legacy\":{\"collections\":false}}","project",["Map",11,12,29,30,56,57,70,71],"project-3",{"id":11,"data":13,"body":17,"filePath":18,"digest":19,"rendered":20},{"title":14,"description":15,"slug":11,"link":16},"Projection-Mapped Interactive String","Using OpenCV and a projector to play nice chords.","https://www.github.com/amjad","# My Project\r\n\r\nThis is a project.","src/data/project/project3.md","5d216fe048071e59",{"html":21,"metadata":22},"\u003Ch1 id=\"my-project\">My Project\u003C/h1>\n\u003Cp>This is a project.\u003C/p>",{"headings":23,"imagePaths":28,"frontmatter":13},[24],{"depth":25,"slug":26,"text":27},1,"my-project","My Project",[],"project-2",{"id":29,"data":31,"body":35,"filePath":36,"digest":37,"rendered":38},{"title":32,"description":33,"slug":29,"link":34},"Autonomous Mario Kart Robot","Driving around a perilous terrain, no one to help...","https://github.com/AmjadYa/Autonomous-Robot","\u003Cdiv class=\"flex gap-2\">\r\n    \u003Cvideo src=\"/videos/robot1.mp4\" muted style=\"max-height:400px ; aspect-ratio:1; object-fit:cover\" controls>\u003C/video>\r\n    \u003Cimg src=\"/images/robot_on_zipline.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\r\n\u003C/div>\r\n\r\n\r\n\r\n## Summary\r\n\r\nIn the summer of 2023, I had to build a robot that could navigate an obstacle course, collect coins off the ground and complete laps for points - with no human interference. Since we thought things weren't hard enough, our team was **one of two** to implement a zipline mechanism to short-cut a part of the course.\r\n\r\nWe split the work up into three disciplines: Electrical, Hardware and Software and progressively integrated components together.\r\n\r\n## Electrical\r\n\r\nThe biggest electrical challenge was getting the right power to all our motors. We had 5V motors, 3V pins, beefy 15V DC motors and an STM-32 Blue Pill (our robot's brain) all supplied from one 15V and one 9V lithium ion battery. We soldered a robust power distribution board with a combination of buck converters and voltage dividers. Due the high currents and magnetic fields caused by the DC motors we had to curl our external wires and take care to avoid noise. \r\n\r\nAnother note is that the Blue Pill has incredibly sensitive pins (!!!). We used our oscilloscope countless times for troubleshooting pin-issues and power failures. I learned how delicate the electronics on low-power systems are.\r\n\r\n\u003Cdiv class=\"flex gap-2\">\r\n    \u003Cimg src=\"/images/h bridge.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\r\n    \u003Cimg src=\"/images/wired up.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\r\n\u003C/div>\r\n\r\n## Hardware\r\n\r\nWe had a drive base that relied on Ackerman steering, with a servo steering the front wheels and DC motors driving the backwheels. The chassis was made out of lasercut plywood, acrylic and 3d printed parts. The zipline mechanism was designed so that the roller wheels interlock into each other like a zipper. This meant the reaction force from contact with the beam would help the claw stay shut. Once the robot touched the ground, that reaction component would disappear and we could safely open the claw again.\r\n\r\n\u003Cdiv class=\"flex gap-2\">\r\n    \u003Cimg src=\"/images/robotcad1.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\r\n    \u003Cimg src=\"/images/robotcad2.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\r\n\u003C/div>\r\n\r\n## Software / Firmware\r\n\r\nThis project was where I fell in love with firmware; it's fun turning abstract instructions into actions.\r\n\r\nWe did everything in Arduino. My favourite functionality was a convolution algorithm that processed input from a 1kHz infrared beacon at the end of the track. This allowed us to detect the light we wanted to follow amidst potential noise and other IR sources. The algo would sample and normalize IR data (from IR sensors attached at the front), then convolve it with a predefined 1kHz wave and threshold the resulting sum to decide if the beacon was detected.\r\n\r\nThe whole robot operated through a multi-stage loop: initially following IR signals then 90 degree turns, PID steering up a ramp, ziplining down and restarting. Additionally, we used hardware interrupts to detect edges and executed maneuvers like backing up or making sharp turns before falling off the edge.\r\n\r\nPutting the code together and getting to see the fruits of all the hardware-labour was satisfying.","src/data/project/project2.md","d24841c2c48732bc",{"html":39,"metadata":40},"\u003Cdiv class=\"flex gap-2\">\n    \u003Cvideo src=\"/videos/robot1.mp4\" muted style=\"max-height:400px ; aspect-ratio:1; object-fit:cover\" controls>\u003C/video>\n    \u003Cimg src=\"/images/robot_on_zipline.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\n\u003C/div>\n\u003Ch2 id=\"summary\">Summary\u003C/h2>\n\u003Cp>In the summer of 2023, I had to build a robot that could navigate an obstacle course, collect coins off the ground and complete laps for points - with no human interference. Since we thought things weren’t hard enough, our team was \u003Cstrong>one of two\u003C/strong> to implement a zipline mechanism to short-cut a part of the course.\u003C/p>\n\u003Cp>We split the work up into three disciplines: Electrical, Hardware and Software and progressively integrated components together.\u003C/p>\n\u003Ch2 id=\"electrical\">Electrical\u003C/h2>\n\u003Cp>The biggest electrical challenge was getting the right power to all our motors. We had 5V motors, 3V pins, beefy 15V DC motors and an STM-32 Blue Pill (our robot’s brain) all supplied from one 15V and one 9V lithium ion battery. We soldered a robust power distribution board with a combination of buck converters and voltage dividers. Due the high currents and magnetic fields caused by the DC motors we had to curl our external wires and take care to avoid noise.\u003C/p>\n\u003Cp>Another note is that the Blue Pill has incredibly sensitive pins (!!!). We used our oscilloscope countless times for troubleshooting pin-issues and power failures. I learned how delicate the electronics on low-power systems are.\u003C/p>\n\u003Cdiv class=\"flex gap-2\">\n    \u003Cimg src=\"/images/h bridge.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\n    \u003Cimg src=\"/images/wired up.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\n\u003C/div>\n\u003Ch2 id=\"hardware\">Hardware\u003C/h2>\n\u003Cp>We had a drive base that relied on Ackerman steering, with a servo steering the front wheels and DC motors driving the backwheels. The chassis was made out of lasercut plywood, acrylic and 3d printed parts. The zipline mechanism was designed so that the roller wheels interlock into each other like a zipper. This meant the reaction force from contact with the beam would help the claw stay shut. Once the robot touched the ground, that reaction component would disappear and we could safely open the claw again.\u003C/p>\n\u003Cdiv class=\"flex gap-2\">\n    \u003Cimg src=\"/images/robotcad1.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\n    \u003Cimg src=\"/images/robotcad2.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\n\u003C/div>\n\u003Ch2 id=\"software--firmware\">Software / Firmware\u003C/h2>\n\u003Cp>This project was where I fell in love with firmware; it’s fun turning abstract instructions into actions.\u003C/p>\n\u003Cp>We did everything in Arduino. My favourite functionality was a convolution algorithm that processed input from a 1kHz infrared beacon at the end of the track. This allowed us to detect the light we wanted to follow amidst potential noise and other IR sources. The algo would sample and normalize IR data (from IR sensors attached at the front), then convolve it with a predefined 1kHz wave and threshold the resulting sum to decide if the beacon was detected.\u003C/p>\n\u003Cp>The whole robot operated through a multi-stage loop: initially following IR signals then 90 degree turns, PID steering up a ramp, ziplining down and restarting. Additionally, we used hardware interrupts to detect edges and executed maneuvers like backing up or making sharp turns before falling off the edge.\u003C/p>\n\u003Cp>Putting the code together and getting to see the fruits of all the hardware-labour was satisfying.\u003C/p>",{"headings":41,"imagePaths":55,"frontmatter":31},[42,46,49,52],{"depth":43,"slug":44,"text":45},2,"summary","Summary",{"depth":43,"slug":47,"text":48},"electrical","Electrical",{"depth":43,"slug":50,"text":51},"hardware","Hardware",{"depth":43,"slug":53,"text":54},"software--firmware","Software / Firmware",[],"project-1",{"id":56,"data":58,"body":62,"filePath":63,"digest":64,"rendered":65},{"title":59,"description":60,"slug":56,"link":61},"CNN Letter Detecting ROS Sim","Teacher kept telling me that I'm just a neural network.","https://github.com/AmjadYa/fizzcomp","\u003Cdiv class=\"flex gap-2\" style=\"justify-content: center ; align-items: center\">\r\n    \u003Cvideo src=\"/videos/CNN competition.MOV\" place-content-center muted style=\"max-height:400px ; object-fit:cover\" controls>\u003C/video>\r\n\u003C/div>\r\n\r\n\u003Ch2>Introduction\u003C/h2>\r\n\r\n\u003Ch3>Background of the Report\u003C/h3>\r\n\u003Cp>\r\nThis project was the culmination of \u003Cstrong>eight\u003C/strong> labs that provided the essential background to develop a fully autonomous, line-following robot for the semester's end competition. This report details the development and implementation of a control system and Convolutional Neural Network (CNN) enabling the robot to detect and read clue plates, navigate a predefined track and avoid obstacles. The labs were very useful! For a personal logbook of all labs and related thoughts, you can click \u003Ca href=\"/public/pdfs/Amjad%20Yaghi%20individual%20logbook.pdf\" target=\"_blank\">here\u003C/a>.\r\n\u003C/p>\r\n\r\n\u003Ch3>Contribution Split\u003C/h3>\r\n\u003Cp>\r\nRichard Helper: PID-based line following and obstacle avoidance.\u003Cbr>\r\nAmjad Yaghi: CNN-based clue detection, visual processing of clue boards, UI development, emergency teleportation strategy for competition.\r\n\u003C/p>\r\n\r\n\u003Ch3>Software Architecture\u003C/h3>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/controller.drawio.png\" alt=\"Driving software architecture\" style=\"max-width:80%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\u003Cp style=\"text-align:center;\">\r\n\u003Ca href=\"https://github.com/OdysseusInSpace/ENPH353_LineFollow\" target=\"_blank\">Driving software architecture\u003C/a>\r\n\u003C/p>\r\n\r\n\u003Cp>\r\n\u003Cimg src=\"/images/gui3.drawio.png\" alt=\"Software architecture used on competition day\" style=\"max-width:80%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\u003Cp style=\"text-align:center;\">\r\n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/tree/main\" target=\"_blank\">Software architecture used on competition day\u003C/a>\r\n\u003C/p>\r\n\r\n\u003Chr />\r\n\r\n\u003Ch2>Discussion\u003C/h2>\r\n\r\n\u003Ch3>Robot Driving Method\u003C/h3>\r\n\u003Cp>\r\nThe planned driving module used PID, a state machine, and several specific behaviours to make its way through the obstacle course. Unfortunately, we were not able to integrate it with the clue detection in time, and so were forced to go through with our emergency controller detailed at the end of the section.\r\n\u003C/p>\r\n\r\n\u003Ch4>Contour Detection and Basic PID\u003C/h4>\r\n\u003Cp>\r\nDue to the roundabout in the middle of the on-road section, the robot needed to be biased toward turning left so as not to collide head on with the truck. There was also the issue of noise in the middle of the road during the dirt phase. To counteract these issues, the PID was based off the centre of the leftmost contour above a certain perimeter threshold, using an adaptive offset towards the opposite end of the screen that grew smaller as the marker was placed closer to the horizon. See the figure below for an example.\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/Dirt_Contour.png\" alt=\"Contour detection and PID marker\" style=\"max-width:50%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\r\n\u003Ch4>Staging\u003C/h4>\r\n\u003Cp>\r\nAs the robot drives, it encounters different scenarios marked by lines across the road. There is a red bar that marks the beginning of the pedestrian section and pink ones that mark the transition between the paved road, dirt road, offroad, and hill sections. The drive module uses a state machine to account for this. Whenever one of these lines is detected – that is, the bottom 10% of the screen has a certain number of pixels that satisfy the red-pink masking – and then disappears, the next state is initialized. This usually means swapping image processing and control algorithms, as is detailed later in this section.\r\n\u003C/p>\r\n\r\n\u003Ch4>NPC Avoidance\u003C/h4>\r\n\u003Cp>\r\nOf the three NPCs, the pedestrian was the only one that the robot would ever hit. The truck acted as a contour in such a way that the PID automatically avoided it, and Baby Yoda was navigated around entirely.\u003Cbr/>\r\nTo dodge the pedestrian, the staging system was put to simple use. Upon crossing the red line, the robot swaps to phase 2, and waits for the marker to shift dramatically. As the pedestrian crosses the left side of the road, it would disrupt the contour and cause this shift. The robot would then resume its normal behaviour.\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/Pedestrian-Readout.png\" alt=\"Readout of pedestrian detection\" style=\"max-width:50%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\r\n\u003Ch4>Offroading\u003C/h4>\r\n\u003Cp>\r\nThe offroad section consists of five simple phases. First, the robot drives forwards at an angle towards the hill for a set period of sim time. It’s not particularly sensitive as to where the robot ends up, so this worked fine. Then the robot would turn until it detected the windows of the car through the blue mask shown in the figure below. These windows are a very particular shade of blue, so it doesn’t get confused by the clue boards. It then PIDs directly toward the centroid of these pixels until they make up a certain percentage of the screen. Finally, it turns right until the pink line is centred in its view, thus catching a view of the clue, and drives towards the tunnel.\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/Blue_Filter.png\" alt=\"Filtering for car windows\" style=\"max-width:40%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\r\n\u003Ch4>Emergency Controller\u003C/h4>\r\n\u003Cp>\r\nThe controller used in the competition does the following:\u003Cbr>\r\nDrive for a predetermined time in predetermined steps, wait for the CNN to report a board, and then continue its circuit. This circuit includes teleportation to each staging line. \r\n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/blob/main/src/controller/src/bismillah_sequence.py\" target=\"_blank\">Here is a link to the python file which contains this sequence.\u003C/a>\r\n\u003C/p>\r\n\r\n\u003Ch3>Clue Plate Recognition Module (CNN)\u003C/h3>\r\n\r\n\u003Ch4>Data Acquisition\u003C/h4>\r\n\u003Cp>\r\nAll data was acquired manually by going into the simulation after changing the clue boards. I would drive around and screenshot them using the GUI. Here’s a demo of what that looked like:\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/usingui.png\" alt=\"GUI showing homography for screenshots\" style=\"max-width:85%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/tree/main/src/controller/saved_images\" target=\"_blank\">This was done tens of times\u003C/a> – and although time consuming – gave us very transferrable (as in: relevant to what we would see on competition day) training data. The GUI has drop-down menus that allow you to control what you want to see in the display labels. This was very useful for testing how much to threshold, erode, dilute, and process images in general.\r\n\u003C/p>\r\n\r\n\u003Ch4>Image Processing\u003C/h4>\r\n\u003Cp>\r\nImages were then meticulously broken down into contours and stretched to get clear pictures of each letter for the CNN to train on / predict. You can read the exact steps \r\n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/blob/main/src/controller/src/prediction_module.py\" target=\"_blank\">here, in the prediction module of the repository.\u003C/a> In short, images were cut in half and a contour box was forcefully drawn around the individual letters. If there was overlap, contours were limited to a certain size and would forcefully split into the best number of boxes in order to separate the letters. \r\n\u003Ca href=\"https://colab.research.google.com/drive/1P0j5OBePBGmmYEO2-LqW9yZ2Fc2BDcIC?usp=sharing\" target=\"_blank\">There is also a more digestible version on Google Colab\u003C/a> which you can play around with and test for yourself.\r\n\u003C/p>\r\n\r\n\u003Ch4>Model Architecture\u003C/h4>\r\n\u003Cp>\r\nThe architecture we used for training was inspired by previous work we did in our labs in class, \r\n\u003Ca href=\"https://docs.google.com/document/d/1fihwZEmhyZtpW0ugc-sJJCCq7CGPNyqaXhZ294aimtY/edit?tab=t.0\" target=\"_blank\">which you can read more about here\u003C/a>. The following is an image of the model summary.\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/modelarch.png\" alt=\"Model architecture summary\" style=\"max-width:70%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\r\n\u003Ch4>Training on Images\u003C/h4>\r\n\u003Cp>\r\nSince we did not do any data augmentation (no artificially generated data based on what we had already), we needed as much of our dataset used for training as possible. As such, the strategy was to monitor training using an 80-20 training-validation split, and once it was satisfactory, to remove the validation set entirely and train on the complete set of images.\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/training.png\" alt=\"Fully using our dataset\" style=\"max-width:70%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\u003Cp>\r\nNotice in the figure above, \u003Ccode>x_train\u003C/code> and \u003Ccode>y_train\u003C/code> are commented out, this is because we were using all letters to train our final competition model. Below is a sample of what our accuracy, loss and confusion matrix looked like when we actually had a split:\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/trainval.png\" alt=\"Training and validation graphs\" style=\"max-width:100%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/confusion.png\" alt=\"Beautiful confusion matrix\" style=\"max-width:100%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\r\n\u003Ch4>Failure Cases\u003C/h4>\r\n\u003Cp>\r\nOccasionally, when segmenting the letters, there is enough overlap to cause a failed prediction. There was an attempt to solve this by intentionally training on overlapped images, however not enough was manually captured. Above, we discussed our performance during competition. Unluckily, one of our predictions failed as a result of this overlap; however, this is a rare event. \r\n\u003Ca href=\"https://drive.google.com/drive/folders/1jROyw9Q_FamL0aAo_l1EG_6aiC61KTwb?usp=sharing\" target=\"_blank\">You can view the individual letters that were used to train the model here\u003C/a>. You may notice some of them have a lot of overlap (by design).\r\n\u003C/p>\r\n\r\n\u003Chr />\r\n\r\n\u003Ch2>Conclusion\u003C/h2>\r\n\r\n\u003Ch3>Performance During Competition\u003C/h3>\r\n\u003Cp>\r\nThe robot performed almost as well as designed during competition. It correctly reported four different clues, teleported three times, and crossed the line once to a total of 18 points. Due to one misspelled clue, we were not able to maximize our points, but there was no catastrophic malfunction.\r\n\u003C/p>\r\n\r\n\u003Ch3>Unadopted Other Attempts\u003C/h3>\r\n\u003Cp>\r\nNotice, in the figure which shows the GUI, that there is a button labeled “Record” with a red light next to it. This was because due to conflicts integrating driving and clue detection together, I tried to quickly create an imitation learning model to navigate the robot. The record button would take screenshots every 100ms and create a CSV file which mapped the current linear and angular velocity of the robot to the image. Images were then fed into \r\n\u003Ca href=\"https://colab.research.google.com/drive/1jRNEESqv6ywCmLLXx__tK_0oSqThVjV7?usp=sharing\" target=\"_blank\">another CNN\u003C/a> with very similar architecture to the one used for training on letters. Surprisingly this method had accurately navigated the first segment of the road up to the second clue board. However, it became quickly apparent that we did not have the necessary compute (shoddy laptop :/) in order to use an imitation learning model in time for the competition, thus it was scrapped and the emergency teleportation method was used instead.\r\n\u003C/p>\r\n\r\n\u003Ch3>Improvements for Future\u003C/h3>\r\n\u003Cp>\r\nThe primary area in need of improvement is the integration of clue detection and driving. We would have used a very wide angled camera so that the clues could be seen without stopping, and altered the homography such that it could undo the resulting distortion. This would also include getting the robot to reach the top of the hill and ironing out the CNN overlap issue. Secondly, there is a lot of potential to optimize the usage of nodes to separate actions which needed to occur simultaneously on the robot.\r\n\u003C/p>\r\n\r\n\u003Chr />\r\n\r\n\u003Ch2>Appendices\u003C/h2>\r\n\r\n\u003Ch3>Referenced Material\u003C/h3>\r\n\u003Cp>\r\nAll referenced material can be found on the ENPH 353 website. There, you will find access to eight labs which helped us in various ways for this project.\r\n\u003Cbr>\u003Cbr>\r\n\u003Ca href=\"https://projectlab.engphys.ubc.ca/enph-353/\" target=\"_blank\">Here’s a link. It will likely update as months pass.\u003C/a>\r\n\u003C/p>\r\n\r\n\u003Ch3>Notable People\u003C/h3>\r\n\u003Cp>\r\nWe would be remiss not to mention Daniel Song, Michael Khoo and Ebrahim Hussain who gave input regarding their previous experience in this course.\u003Cbr>\u003Cbr>\r\nLastly, many discussions were had with Ella Yan (a colleague taking the course) particularly regarding image processing and how to implement an imitation learning model. We would like to give our flowers to her and her teammate Nora Shao for managing to implement their imitation learning model with severe time constraints.\r\n\u003C/p>\r\n\r\n\u003Ch3>ChatGPT Usage\u003C/h3>\r\n\u003Cp>\r\nChatGPT was used to help implement ideas that we already had, and for debugging. Here is an example of how ChatGPT was used to help us troubleshoot making predictions.\r\n\u003C/p>\r\n\u003Cp>\r\n\u003Cimg src=\"/images/tensor.png\" alt=\"Troubleshooting with ChatGPT\" style=\"max-width:70%; display:block; margin:auto;\" />\r\n\u003C/p>\r\n\u003Cp>\r\nIn fact, this was a notable conversation with ChatGPT, as it helped to reveal that there was a mismatch between the version of the model in Google Colab and the one used locally! (Took a while to figure out what was wrong.)\r\n\u003C/p>","src/data/project/project1.md","c876e0e5b03c6abf",{"html":66,"metadata":67},"\u003Cdiv class=\"flex gap-2\" style=\"justify-content: center ; align-items: center\">\n    \u003Cvideo src=\"/videos/CNN competition.MOV\" place-content-center=\"\" muted style=\"max-height:400px ; object-fit:cover\" controls>\u003C/video>\n\u003C/div>\n\u003Ch2>Introduction\u003C/h2>\n\u003Ch3>Background of the Report\u003C/h3>\n\u003Cp>\nThis project was the culmination of \u003Cstrong>eight\u003C/strong> labs that provided the essential background to develop a fully autonomous, line-following robot for the semester's end competition. This report details the development and implementation of a control system and Convolutional Neural Network (CNN) enabling the robot to detect and read clue plates, navigate a predefined track and avoid obstacles. The labs were very useful! For a personal logbook of all labs and related thoughts, you can click \u003Ca href=\"/public/pdfs/Amjad%20Yaghi%20individual%20logbook.pdf\" target=\"_blank\">here\u003C/a>.\n\u003C/p>\n\u003Ch3>Contribution Split\u003C/h3>\n\u003Cp>\nRichard Helper: PID-based line following and obstacle avoidance.\u003Cbr>\nAmjad Yaghi: CNN-based clue detection, visual processing of clue boards, UI development, emergency teleportation strategy for competition.\n\u003C/p>\n\u003Ch3>Software Architecture\u003C/h3>\n\u003Cp>\n\u003Cimg src=\"/images/controller.drawio.png\" alt=\"Driving software architecture\" style=\"max-width:80%; display:block; margin:auto;\">\n\u003C/p>\n\u003Cp style=\"text-align:center;\">\n\u003Ca href=\"https://github.com/OdysseusInSpace/ENPH353_LineFollow\" target=\"_blank\">Driving software architecture\u003C/a>\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/gui3.drawio.png\" alt=\"Software architecture used on competition day\" style=\"max-width:80%; display:block; margin:auto;\">\n\u003C/p>\n\u003Cp style=\"text-align:center;\">\n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/tree/main\" target=\"_blank\">Software architecture used on competition day\u003C/a>\n\u003C/p>\n\u003Chr>\n\u003Ch2>Discussion\u003C/h2>\n\u003Ch3>Robot Driving Method\u003C/h3>\n\u003Cp>\nThe planned driving module used PID, a state machine, and several specific behaviours to make its way through the obstacle course. Unfortunately, we were not able to integrate it with the clue detection in time, and so were forced to go through with our emergency controller detailed at the end of the section.\n\u003C/p>\n\u003Ch4>Contour Detection and Basic PID\u003C/h4>\n\u003Cp>\nDue to the roundabout in the middle of the on-road section, the robot needed to be biased toward turning left so as not to collide head on with the truck. There was also the issue of noise in the middle of the road during the dirt phase. To counteract these issues, the PID was based off the centre of the leftmost contour above a certain perimeter threshold, using an adaptive offset towards the opposite end of the screen that grew smaller as the marker was placed closer to the horizon. See the figure below for an example.\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/Dirt_Contour.png\" alt=\"Contour detection and PID marker\" style=\"max-width:50%; display:block; margin:auto;\">\n\u003C/p>\n\u003Ch4>Staging\u003C/h4>\n\u003Cp>\nAs the robot drives, it encounters different scenarios marked by lines across the road. There is a red bar that marks the beginning of the pedestrian section and pink ones that mark the transition between the paved road, dirt road, offroad, and hill sections. The drive module uses a state machine to account for this. Whenever one of these lines is detected – that is, the bottom 10% of the screen has a certain number of pixels that satisfy the red-pink masking – and then disappears, the next state is initialized. This usually means swapping image processing and control algorithms, as is detailed later in this section.\n\u003C/p>\n\u003Ch4>NPC Avoidance\u003C/h4>\n\u003Cp>\nOf the three NPCs, the pedestrian was the only one that the robot would ever hit. The truck acted as a contour in such a way that the PID automatically avoided it, and Baby Yoda was navigated around entirely.\u003Cbr>\nTo dodge the pedestrian, the staging system was put to simple use. Upon crossing the red line, the robot swaps to phase 2, and waits for the marker to shift dramatically. As the pedestrian crosses the left side of the road, it would disrupt the contour and cause this shift. The robot would then resume its normal behaviour.\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/Pedestrian-Readout.png\" alt=\"Readout of pedestrian detection\" style=\"max-width:50%; display:block; margin:auto;\">\n\u003C/p>\n\u003Ch4>Offroading\u003C/h4>\n\u003Cp>\nThe offroad section consists of five simple phases. First, the robot drives forwards at an angle towards the hill for a set period of sim time. It’s not particularly sensitive as to where the robot ends up, so this worked fine. Then the robot would turn until it detected the windows of the car through the blue mask shown in the figure below. These windows are a very particular shade of blue, so it doesn’t get confused by the clue boards. It then PIDs directly toward the centroid of these pixels until they make up a certain percentage of the screen. Finally, it turns right until the pink line is centred in its view, thus catching a view of the clue, and drives towards the tunnel.\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/Blue_Filter.png\" alt=\"Filtering for car windows\" style=\"max-width:40%; display:block; margin:auto;\">\n\u003C/p>\n\u003Ch4>Emergency Controller\u003C/h4>\n\u003Cp>\nThe controller used in the competition does the following:\u003Cbr>\nDrive for a predetermined time in predetermined steps, wait for the CNN to report a board, and then continue its circuit. This circuit includes teleportation to each staging line. \n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/blob/main/src/controller/src/bismillah_sequence.py\" target=\"_blank\">Here is a link to the python file which contains this sequence.\u003C/a>\n\u003C/p>\n\u003Ch3>Clue Plate Recognition Module (CNN)\u003C/h3>\n\u003Ch4>Data Acquisition\u003C/h4>\n\u003Cp>\nAll data was acquired manually by going into the simulation after changing the clue boards. I would drive around and screenshot them using the GUI. Here’s a demo of what that looked like:\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/usingui.png\" alt=\"GUI showing homography for screenshots\" style=\"max-width:85%; display:block; margin:auto;\">\n\u003C/p>\n\u003Cp>\n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/tree/main/src/controller/saved_images\" target=\"_blank\">This was done tens of times\u003C/a> – and although time consuming – gave us very transferrable (as in: relevant to what we would see on competition day) training data. The GUI has drop-down menus that allow you to control what you want to see in the display labels. This was very useful for testing how much to threshold, erode, dilute, and process images in general.\n\u003C/p>\n\u003Ch4>Image Processing\u003C/h4>\n\u003Cp>\nImages were then meticulously broken down into contours and stretched to get clear pictures of each letter for the CNN to train on / predict. You can read the exact steps \n\u003Ca href=\"https://github.com/AmjadYa/fizzcomp/blob/main/src/controller/src/prediction_module.py\" target=\"_blank\">here, in the prediction module of the repository.\u003C/a> In short, images were cut in half and a contour box was forcefully drawn around the individual letters. If there was overlap, contours were limited to a certain size and would forcefully split into the best number of boxes in order to separate the letters. \n\u003Ca href=\"https://colab.research.google.com/drive/1P0j5OBePBGmmYEO2-LqW9yZ2Fc2BDcIC?usp=sharing\" target=\"_blank\">There is also a more digestible version on Google Colab\u003C/a> which you can play around with and test for yourself.\n\u003C/p>\n\u003Ch4>Model Architecture\u003C/h4>\n\u003Cp>\nThe architecture we used for training was inspired by previous work we did in our labs in class, \n\u003Ca href=\"https://docs.google.com/document/d/1fihwZEmhyZtpW0ugc-sJJCCq7CGPNyqaXhZ294aimtY/edit?tab=t.0\" target=\"_blank\">which you can read more about here\u003C/a>. The following is an image of the model summary.\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/modelarch.png\" alt=\"Model architecture summary\" style=\"max-width:70%; display:block; margin:auto;\">\n\u003C/p>\n\u003Ch4>Training on Images\u003C/h4>\n\u003Cp>\nSince we did not do any data augmentation (no artificially generated data based on what we had already), we needed as much of our dataset used for training as possible. As such, the strategy was to monitor training using an 80-20 training-validation split, and once it was satisfactory, to remove the validation set entirely and train on the complete set of images.\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/training.png\" alt=\"Fully using our dataset\" style=\"max-width:70%; display:block; margin:auto;\">\n\u003C/p>\n\u003Cp>\nNotice in the figure above, \u003Ccode>x_train\u003C/code> and \u003Ccode>y_train\u003C/code> are commented out, this is because we were using all letters to train our final competition model. Below is a sample of what our accuracy, loss and confusion matrix looked like when we actually had a split:\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/trainval.png\" alt=\"Training and validation graphs\" style=\"max-width:100%; display:block; margin:auto;\">\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/confusion.png\" alt=\"Beautiful confusion matrix\" style=\"max-width:100%; display:block; margin:auto;\">\n\u003C/p>\n\u003Ch4>Failure Cases\u003C/h4>\n\u003Cp>\nOccasionally, when segmenting the letters, there is enough overlap to cause a failed prediction. There was an attempt to solve this by intentionally training on overlapped images, however not enough was manually captured. Above, we discussed our performance during competition. Unluckily, one of our predictions failed as a result of this overlap; however, this is a rare event. \n\u003Ca href=\"https://drive.google.com/drive/folders/1jROyw9Q_FamL0aAo_l1EG_6aiC61KTwb?usp=sharing\" target=\"_blank\">You can view the individual letters that were used to train the model here\u003C/a>. You may notice some of them have a lot of overlap (by design).\n\u003C/p>\n\u003Chr>\n\u003Ch2>Conclusion\u003C/h2>\n\u003Ch3>Performance During Competition\u003C/h3>\n\u003Cp>\nThe robot performed almost as well as designed during competition. It correctly reported four different clues, teleported three times, and crossed the line once to a total of 18 points. Due to one misspelled clue, we were not able to maximize our points, but there was no catastrophic malfunction.\n\u003C/p>\n\u003Ch3>Unadopted Other Attempts\u003C/h3>\n\u003Cp>\nNotice, in the figure which shows the GUI, that there is a button labeled “Record” with a red light next to it. This was because due to conflicts integrating driving and clue detection together, I tried to quickly create an imitation learning model to navigate the robot. The record button would take screenshots every 100ms and create a CSV file which mapped the current linear and angular velocity of the robot to the image. Images were then fed into \n\u003Ca href=\"https://colab.research.google.com/drive/1jRNEESqv6ywCmLLXx__tK_0oSqThVjV7?usp=sharing\" target=\"_blank\">another CNN\u003C/a> with very similar architecture to the one used for training on letters. Surprisingly this method had accurately navigated the first segment of the road up to the second clue board. However, it became quickly apparent that we did not have the necessary compute (shoddy laptop :/) in order to use an imitation learning model in time for the competition, thus it was scrapped and the emergency teleportation method was used instead.\n\u003C/p>\n\u003Ch3>Improvements for Future\u003C/h3>\n\u003Cp>\nThe primary area in need of improvement is the integration of clue detection and driving. We would have used a very wide angled camera so that the clues could be seen without stopping, and altered the homography such that it could undo the resulting distortion. This would also include getting the robot to reach the top of the hill and ironing out the CNN overlap issue. Secondly, there is a lot of potential to optimize the usage of nodes to separate actions which needed to occur simultaneously on the robot.\n\u003C/p>\n\u003Chr>\n\u003Ch2>Appendices\u003C/h2>\n\u003Ch3>Referenced Material\u003C/h3>\n\u003Cp>\nAll referenced material can be found on the ENPH 353 website. There, you will find access to eight labs which helped us in various ways for this project.\n\u003Cbr>\u003Cbr>\n\u003Ca href=\"https://projectlab.engphys.ubc.ca/enph-353/\" target=\"_blank\">Here’s a link. It will likely update as months pass.\u003C/a>\n\u003C/p>\n\u003Ch3>Notable People\u003C/h3>\n\u003Cp>\nWe would be remiss not to mention Daniel Song, Michael Khoo and Ebrahim Hussain who gave input regarding their previous experience in this course.\u003Cbr>\u003Cbr>\nLastly, many discussions were had with Ella Yan (a colleague taking the course) particularly regarding image processing and how to implement an imitation learning model. We would like to give our flowers to her and her teammate Nora Shao for managing to implement their imitation learning model with severe time constraints.\n\u003C/p>\n\u003Ch3>ChatGPT Usage\u003C/h3>\n\u003Cp>\nChatGPT was used to help implement ideas that we already had, and for debugging. Here is an example of how ChatGPT was used to help us troubleshoot making predictions.\n\u003C/p>\n\u003Cp>\n\u003Cimg src=\"/images/tensor.png\" alt=\"Troubleshooting with ChatGPT\" style=\"max-width:70%; display:block; margin:auto;\">\n\u003C/p>\n\u003Cp>\nIn fact, this was a notable conversation with ChatGPT, as it helped to reveal that there was a mismatch between the version of the model in Google Colab and the one used locally! (Took a while to figure out what was wrong.)\n\u003C/p>",{"headings":68,"imagePaths":69,"frontmatter":58},[],[],"project-4",{"id":70,"data":72,"body":76,"filePath":77,"digest":78,"rendered":79},{"title":73,"description":74,"slug":70,"link":75},"Breaking Blood Clots in Zero G","Studying clot behaviour for future space mission safety.","https://github.com/AmjadYa/","\u003Cimg class=\"mx-auto\" src=\"/images/UBCRocket_TeamPhoto.JPG\" style=\"max-height:400px ; object-fit:cover\">\r\n\r\n## What happened to you?!\r\n\r\nThey told me it would be the hardest semester of my life. (They were right.)\r\n\r\nIn the summer of 2023, I had to build a robot that could navigate an obstacle course, collect coins off the ground and complete laps for points with no human interference. Since we thought things weren't hard enough, our team was **one of two** to use the zipline to short-cut a part of the course.\r\n\r\nWe split the work up into three disciplines: Electrical, Hardware and Software and progressively integrated components together.\r\n\r\n## Electrical\r\n\r\n\u003Cdiv class=\"flex gap-2\">\r\n    \u003Cimg src=\"/images/h bridge.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\r\n    \u003Cimg src=\"/images/wired up.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\r\n\u003C/div>\r\n\r\n## Hardware\r\n\r\nWe had a drive base that relied on Ackerman steering, with a servo steering front wheels and DC motors driving backwheels. The chassis was made out of lasercut plywood and acrylic and 3d printed parts. The zipline mechanism was designed so that the roller wheels interlock into each other like a zipper. This meant the reaction force from contact with the beam would help the claw stay shut. Once the robot touched the ground, that reaction component would disappear and we could safely open the claw again.\r\n\r\n\u003Cdiv class=\"flex gap-2\">\r\n    \u003Cimg src=\"/images/robotcad1.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\r\n    \u003Cimg src=\"/images/robotcad2.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\r\n\u003C/div>\r\n\r\n## Software / Firmware\r\n\r\nThis project was where I fell in love with firmware; it's fun turning abstract instructions into actions.\r\n\r\nWe did everything in Arduino. My favourite functionality was a convolution algorithm that processed input from a 1kHz infrared beacon at the end of the track. This allowed us to detect the light we wanted to follow amidst potential noise and other IR sources. The algo would sample and normalize IR data (from IR sensors attached at the front), then convolve it with a predefined 1kHz and threshold the resulting sum to decide if the beacon was detected.\r\n\r\nThe whole robot operated through a multi-stage loop: initially following IR signals then hard-coded 90 degree turns, PID steering up a ramp, ziplining down and restarting. Additionally, we used hardware interrupts to detect edges and executed maneuvers like backing up or making sharp turns before falling off the edge.\r\n\r\nPutting the code together and getting to see the fruits of all the hardware-labour was satisfying.\r\n\r\n\u003Csmall>Also... I wasn't very good at OOP yet so all the code was in one file. I'm better now I promise.\u003C/small>","src/data/project/project4.md","cd2a6f42a9dc8d0c",{"html":80,"metadata":81},"\u003Cimg class=\"mx-auto\" src=\"/images/UBCRocket_TeamPhoto.JPG\" style=\"max-height:400px ; object-fit:cover\">\n\u003Ch2 id=\"what-happened-to-you\">What happened to you?!\u003C/h2>\n\u003Cp>They told me it would be the hardest semester of my life. (They were right.)\u003C/p>\n\u003Cp>In the summer of 2023, I had to build a robot that could navigate an obstacle course, collect coins off the ground and complete laps for points with no human interference. Since we thought things weren’t hard enough, our team was \u003Cstrong>one of two\u003C/strong> to use the zipline to short-cut a part of the course.\u003C/p>\n\u003Cp>We split the work up into three disciplines: Electrical, Hardware and Software and progressively integrated components together.\u003C/p>\n\u003Ch2 id=\"electrical\">Electrical\u003C/h2>\n\u003Cdiv class=\"flex gap-2\">\n    \u003Cimg src=\"/images/h bridge.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\n    \u003Cimg src=\"/images/wired up.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\n\u003C/div>\n\u003Ch2 id=\"hardware\">Hardware\u003C/h2>\n\u003Cp>We had a drive base that relied on Ackerman steering, with a servo steering front wheels and DC motors driving backwheels. The chassis was made out of lasercut plywood and acrylic and 3d printed parts. The zipline mechanism was designed so that the roller wheels interlock into each other like a zipper. This meant the reaction force from contact with the beam would help the claw stay shut. Once the robot touched the ground, that reaction component would disappear and we could safely open the claw again.\u003C/p>\n\u003Cdiv class=\"flex gap-2\">\n    \u003Cimg src=\"/images/robotcad1.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\n    \u003Cimg src=\"/images/robotcad2.jpg\" style=\"max-height:400px ; aspect-ratio:1 ; object-fit:cover\">\n\u003C/div>\n\u003Ch2 id=\"software--firmware\">Software / Firmware\u003C/h2>\n\u003Cp>This project was where I fell in love with firmware; it’s fun turning abstract instructions into actions.\u003C/p>\n\u003Cp>We did everything in Arduino. My favourite functionality was a convolution algorithm that processed input from a 1kHz infrared beacon at the end of the track. This allowed us to detect the light we wanted to follow amidst potential noise and other IR sources. The algo would sample and normalize IR data (from IR sensors attached at the front), then convolve it with a predefined 1kHz and threshold the resulting sum to decide if the beacon was detected.\u003C/p>\n\u003Cp>The whole robot operated through a multi-stage loop: initially following IR signals then hard-coded 90 degree turns, PID steering up a ramp, ziplining down and restarting. Additionally, we used hardware interrupts to detect edges and executed maneuvers like backing up or making sharp turns before falling off the edge.\u003C/p>\n\u003Cp>Putting the code together and getting to see the fruits of all the hardware-labour was satisfying.\u003C/p>\n\u003Cp>\u003Csmall>Also… I wasn’t very good at OOP yet so all the code was in one file. I’m better now I promise.\u003C/small>\u003C/p>",{"headings":82,"imagePaths":89,"frontmatter":72},[83,86,87,88],{"depth":43,"slug":84,"text":85},"what-happened-to-you","What happened to you?!",{"depth":43,"slug":47,"text":48},{"depth":43,"slug":50,"text":51},{"depth":43,"slug":53,"text":54},[]]